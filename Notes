ТЗ – Парсер.
Цель парсера – По запросу выдавать HR-у список телефонных номеров из базы резюме

    1) Я задаю поиск на хедхантере (далее - ХХ), устанавливаю фильтры и ключевые слова, например хочу найти питониста, со знанием Django, алгоритмов, опытом с многопоточностью, в Москве и т.д.
    2) Мне база ХХ выдает список условно из 100 резюме. Я хочу, чтобы парсер доставал из этих 100 резюме список из 100 телефонных номеров соответственно.
    3) Как это точно должно выглядеть не особо сейчас важно. Например, это будет расширение в браузере, с небольшим интерфейсом, которое по кнопке спарсит и выведет в интерфейсе мне список – Имя Фамилия, Номер телефона. Возможно помимо этих данных в конце будет ссылка на резюме, как идея.
    4) Либо, вместо расширения в браузере, создается на выходе Excel таблица, где просто будет представлен список – Имя Фамилия, Номер Телефона, и наверное ссылка на резюме с условного ХХ.

Это что касается идеи и цели :) 
Как это должно выглядеть в бекенде – как HR сказать не могу)


Что получилось:
1. Доступ к спискам резюме с полной информацией (включая ФИО и контакты) доступен после авторизации и платный.

2. Как приделать к этому всему авторизацию - я пока не разобралась, не было подобного опыта. 

3. В папке parserBeautifulSoup файлы с парсерами для сайтов hh.ru, zarplata.ru, rabota.ru (hh.py, zarp.py, rabota.py соответственно). В этих файлах использована библиотека BeautifulSoup, с помощью которой информация на странице ищется по тегам и классам в html коде страницы. В этих файлах для примера из списка резюме вытаскивается пол, возраст, название вакансии, которую указал соискатель, и ссылка на само резюме, записывается в csv файл. 
	В файлах hh_auth.py, zarp_auth.py, rabota_auth.py - то же самое, но ищется имя, фамилия и номер телефона соискателя. Теги и классы для поиска этих данных я брала из своего резюме, поэтому не уверена, что поиск настроен верно, т.к. не знаю, как выглядит резюме "глазами HRа" и его доступом. 
	
4. В папке  parserAPI - через классы. Переделан из парсера вакансий, с вакансиями работал, с резюме - нет возможности проверить, т.к. опять же нужна атворизация и платный доступ. Пока только для сайта hh.ru, но сайт  zarplata.ru максимально идентичный, так что в коде можно только url поменять. 

Вывод: парсер через BeautifulSoup - проверен, работает, приделать авторизацию, немного поднастроить поиск и будет работать. Но медленно. (Возможно дело в характеристиках моего компа или скорости интернета, но вытащить и записать в файл данные из 20 резюме занимало примерно 20-30 сек). Парсер через API подозреваю должен использовать меньше времени и памати, но проверить его работоспособность у меня нет доступа.
